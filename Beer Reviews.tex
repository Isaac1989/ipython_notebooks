
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass{article}

    
    
    \usepackage{graphicx} % Used to insert images
    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{color} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    

    
    
    \definecolor{orange}{cmyk}{0,0.4,0.8,0.2}
    \definecolor{darkorange}{rgb}{.71,0.21,0.01}
    \definecolor{darkgreen}{rgb}{.12,.54,.11}
    \definecolor{myteal}{rgb}{.26, .44, .56}
    \definecolor{gray}{gray}{0.45}
    \definecolor{lightgray}{gray}{.95}
    \definecolor{mediumgray}{gray}{.8}
    \definecolor{inputbackground}{rgb}{.95, .95, .85}
    \definecolor{outputbackground}{rgb}{.95, .95, .95}
    \definecolor{traceback}{rgb}{1, .95, .95}
    % ansi colors
    \definecolor{red}{rgb}{.6,0,0}
    \definecolor{green}{rgb}{0,.65,0}
    \definecolor{brown}{rgb}{0.6,0.6,0}
    \definecolor{blue}{rgb}{0,.145,.698}
    \definecolor{purple}{rgb}{.698,.145,.698}
    \definecolor{cyan}{rgb}{0,.698,.698}
    \definecolor{lightgray}{gray}{0.5}
    
    % bright ansi colors
    \definecolor{darkgray}{gray}{0.25}
    \definecolor{lightred}{rgb}{1.0,0.39,0.28}
    \definecolor{lightgreen}{rgb}{0.48,0.99,0.0}
    \definecolor{lightblue}{rgb}{0.53,0.81,0.92}
    \definecolor{lightpurple}{rgb}{0.87,0.63,0.87}
    \definecolor{lightcyan}{rgb}{0.5,1.0,0.83}
    
    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Beer Reviews}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=blue,
      linkcolor=darkorange,
      citecolor=darkgreen,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{Introduction}\label{introduction}

In this notebook, I'll demonstrate some Data Science applications on a
fun dataset. The dataset consists of a series of beer ratings sampled
from the \href{http://www.beeradvocate.com}{Beer Advocate} database.
(Note that this data \emph{used to be} available
\href{http://snap.stanford.edu/data/web-BeerAdvocate.html}{here}, but
has recently been removed, by request of Beer Advocate). The dataset has
approx. 1.5 million reviews of tens of thousands of beers from tens of
thousands of users. Each beer review is from a single user and contains
ratings of the beer's palate, taste, ABV, aroma, appearance, and overall
rating (all from 0-5).

On this dataset, I'll demonstrate two recommendation system examples,
one using content-based filtering, and another using collaborative
filtering. The third example will demonstrate building predictive models
of overall beer ratings. The analysis (obviously, given that I'm using
IPython notebook) will be in python. The main python modules that I will
use are Pandas (for data munging), Numpy (for number crunching),
Matplotlib (for visualization), scikits-learn (for modeling) and pyrsvd
(for fancy matrix completion used in the collaborative filtering
example).

First, some setup and loading the data\ldots{}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{c}{\PYZsh{} PREAMBLE/SETUP}
        \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k+kn}{as} \PY{n+nn}{pd}
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k+kn}{as} \PY{n+nn}{np}
        \PY{k+kn}{from} \PY{n+nn}{matplotlib} \PY{k+kn}{import} \PY{n}{pyplot} \PY{k}{as} \PY{n}{plt}
        
        \PY{c}{\PYZsh{} DISPLAY FONTS}
        \PY{n}{font} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s}{\PYZsq{}}\PY{l+s}{family}\PY{l+s}{\PYZsq{}} \PY{p}{:} \PY{l+s}{\PYZsq{}}\PY{l+s}{arial}\PY{l+s}{\PYZsq{}}\PY{p}{,}
                \PY{l+s}{\PYZsq{}}\PY{l+s}{weight}\PY{l+s}{\PYZsq{}} \PY{p}{:} \PY{l+s}{\PYZsq{}}\PY{l+s}{bold}\PY{l+s}{\PYZsq{}}\PY{p}{,}
                \PY{l+s}{\PYZsq{}}\PY{l+s}{size}\PY{l+s}{\PYZsq{}}   \PY{p}{:} \PY{l+m+mi}{16}\PY{p}{\PYZcb{}}
        \PY{n}{matplotlib}\PY{o}{.}\PY{n}{rc}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{font}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{o}{*}\PY{o}{*}\PY{n}{font}\PY{p}{)}
        
        \PY{c}{\PYZsh{} PATH TO DATA...}
        \PY{n}{df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{/home/dustin/data/toy\PYZus{}datasets/beer\PYZus{}reviews/beer\PYZus{}reviews.csv}\PY{l+s}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

    \section{Example I: Content-based
Filtering}\label{example-i-content-based-filtering}

In most cases, content-based filtering compares products based on
similarity metrics, suggesting products that are similar to one that was
chosen by a user. Here, I'll demonstrate content-based filtering on the
beer dataset.

    \subsection{Let's compare some beers}\label{lets-compare-some-beers}

As mentioned above, the database contains around 1.5 million beer
reviews for thousands of beers and reviewers. To keep the demonstration
simple and properly scaled for a laptop, I'll choose a popular subset of
all the available beers. (For larger scale problems, like analyzing the
original or bigger databases, we would parallelize a lot of these
calculations.)

We'll investigate the following 11 popular beers:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{c}{\PYZsh{} DEFINE A SET OF BEERS TO COMPARE}
        \PY{c}{\PYZsh{} (FOR ALL BEERS IN DATASET, WE WOULD WANT TO PARALLELIZE)}
        \PY{n}{beers} \PY{o}{=} \PY{p}{[}\PY{l+s}{\PYZdq{}}\PY{l+s}{Dale}\PY{l+s}{\PYZsq{}}\PY{l+s}{s Pale Ale}\PY{l+s}{\PYZdq{}}\PY{p}{,} 
                 \PY{l+s}{\PYZdq{}}\PY{l+s}{Sierra Nevada Pale Ale}\PY{l+s}{\PYZdq{}}\PY{p}{,} 
                 \PY{l+s}{\PYZdq{}}\PY{l+s}{Michelob Ultra}\PY{l+s}{\PYZdq{}}\PY{p}{,}
                 \PY{l+s}{\PYZdq{}}\PY{l+s}{Two Hearted Ale}\PY{l+s}{\PYZdq{}}\PY{p}{,} 
                 \PY{l+s}{\PYZdq{}}\PY{l+s}{Natural Light}\PY{l+s}{\PYZdq{}}\PY{p}{,} 
                 \PY{l+s}{\PYZdq{}}\PY{l+s}{Bud Light}\PY{l+s}{\PYZdq{}}\PY{p}{,} 
                 \PY{l+s}{\PYZdq{}}\PY{l+s}{Fat Tire Amber Ale}\PY{l+s}{\PYZdq{}}\PY{p}{,} 
                 \PY{l+s}{\PYZdq{}}\PY{l+s}{Coors Light}\PY{l+s}{\PYZdq{}}\PY{p}{,}
                 \PY{l+s}{\PYZdq{}}\PY{l+s}{Blue Moon Belgian White}\PY{l+s}{\PYZdq{}}\PY{p}{,} 
                 \PY{l+s}{\PYZdq{}}\PY{l+s}{90 Minute IPA}\PY{l+s}{\PYZdq{}}\PY{p}{,} 
                 \PY{l+s}{\PYZdq{}}\PY{l+s}{Guinness Draught}\PY{l+s}{\PYZdq{}}\PY{p}{]}
\end{Verbatim}

    In order to compare any two of these beers, we need an objective
definition of similarity. One way of defining similarity is by
calculating the (Euclidian) distance between features (e.g.~ABV or
palate ratings) of the two beers. To do so, I define a function
\textbf{feature\_distances} that calculates the distance between two
beers' palate, aroma, taste, and ABV. For each feature, the distance is
calculated across all reviewers that reviewed both beers. (The function
also uses helper function for extracting beer reviews)

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{k}{def} \PY{n+nf}{get\PYZus{}beer\PYZus{}reviews}\PY{p}{(}\PY{n}{beer}\PY{p}{,} \PY{n}{reviewers}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}Extract review given a list of overlapping reviewers\PYZsq{}\PYZsq{}\PYZsq{}}
            \PY{n}{mask} \PY{o}{=} \PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{review\PYZus{}profilename}\PY{o}{.}\PY{n}{isin}\PY{p}{(}\PY{n}{reviewers}\PY{p}{)}\PY{p}{)} \PY{o}{\PYZam{}} \PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{beer\PYZus{}name} \PY{o}{==} \PY{n}{beer}\PY{p}{)}
            \PY{n}{reviews} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{n}{mask}\PY{p}{]}\PY{o}{.}\PY{n}{sort}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{review\PYZus{}profilename}\PY{l+s}{\PYZsq{}}\PY{p}{)}
            \PY{n}{reviews} \PY{o}{=} \PY{n}{reviews}\PY{p}{[}\PY{n}{reviews}\PY{o}{.}\PY{n}{review\PYZus{}profilename}\PY{o}{.}\PY{n}{duplicated}\PY{p}{(}\PY{p}{)} \PY{o}{==} \PY{n+nb+bp}{False}\PY{p}{]} \PY{c}{\PYZsh{} GET RID OF REPLICATED REVIEWS}
            \PY{k}{return} \PY{n}{reviews}
        
        \PY{k}{def} \PY{n+nf}{feature\PYZus{}distances}\PY{p}{(}\PY{n}{beer1}\PY{p}{,}\PY{n}{beer2}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}Calculate similarities between a set of features for beer1 and beer2\PYZsq{}\PYZsq{}\PYZsq{}}    
            \PY{k+kn}{from} \PY{n+nn}{sklearn.metrics.pairwise} \PY{k+kn}{import} \PY{n}{euclidean\PYZus{}distances}
                
            \PY{c}{\PYZsh{} GET REVIEWERS FOR EACH BEER}
            \PY{n}{beer\PYZus{}1\PYZus{}reviewers} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{n}{df}\PY{o}{.}\PY{n}{beer\PYZus{}name} \PY{o}{==} \PY{n}{beer1}\PY{p}{]}\PY{o}{.}\PY{n}{review\PYZus{}profilename}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{p}{)}
            \PY{n}{beer\PYZus{}2\PYZus{}reviewers} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{n}{df}\PY{o}{.}\PY{n}{beer\PYZus{}name} \PY{o}{==} \PY{n}{beer2}\PY{p}{]}\PY{o}{.}\PY{n}{review\PYZus{}profilename}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{p}{)}
            
            \PY{c}{\PYZsh{} TAKE THE INTERSECTION}
            \PY{n}{common\PYZus{}reviewers} \PY{o}{=} \PY{n+nb}{set}\PY{p}{(}\PY{n}{beer\PYZus{}1\PYZus{}reviewers}\PY{p}{)}\PY{o}{.}\PY{n}{intersection}\PY{p}{(}\PY{n}{beer\PYZus{}2\PYZus{}reviewers}\PY{p}{)}
            
            \PY{c}{\PYZsh{} EXTRACT REVIEWS}
            \PY{n}{revs1} \PY{o}{=} \PY{n}{get\PYZus{}beer\PYZus{}reviews}\PY{p}{(}\PY{n}{beer1}\PY{p}{,} \PY{n}{common\PYZus{}reviewers}\PY{p}{)}
            \PY{n}{revs2} \PY{o}{=} \PY{n}{get\PYZus{}beer\PYZus{}reviews}\PY{p}{(}\PY{n}{beer2}\PY{p}{,} \PY{n}{common\PYZus{}reviewers}\PY{p}{)}
               
            \PY{c}{\PYZsh{} CALCULATE THE FEATURE DISTANCES}
            \PY{n}{dist} \PY{o}{=} \PY{p}{[}\PY{p}{]}
            \PY{n}{features} \PY{o}{=} \PY{p}{[}\PY{l+s}{\PYZsq{}}\PY{l+s}{review\PYZus{}palate}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{review\PYZus{}aroma}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{review\PYZus{}taste}\PY{l+s}{\PYZsq{}}\PY{p}{,}\PY{l+s}{\PYZsq{}}\PY{l+s}{beer\PYZus{}abv}\PY{l+s}{\PYZsq{}}\PY{p}{]}
            \PY{k}{for} \PY{n}{f} \PY{o+ow}{in} \PY{n}{features}\PY{p}{:}
                \PY{n}{dist}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{euclidean\PYZus{}distances}\PY{p}{(}\PY{n}{revs1}\PY{p}{[}\PY{n}{f}\PY{p}{]}\PY{p}{,}\PY{n}{revs2}\PY{p}{[}\PY{n}{f}\PY{p}{]}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
                        
            \PY{k}{return} \PY{n}{dist}
\end{Verbatim}

    Now that we have a way of calculating the distance between two beers, we
can compare each of the common beers defined above\ldots{}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{c}{\PYZsh{} CALCULATE SIMILARITIES BETWEEN EACH OF THE COMMON BEERS}
        \PY{n}{sims} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{k}{for} \PY{n}{ii}\PY{p}{,} \PY{n}{b1} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{beers}\PY{p}{)}\PY{p}{:}
            \PY{k}{print} \PY{l+s}{\PYZsq{}}\PY{l+s}{Calculating similarities for beer (}\PY{l+s+si}{\PYZpc{}d}\PY{l+s}{ / }\PY{l+s+si}{\PYZpc{}d}\PY{l+s}{): }\PY{l+s+si}{\PYZpc{}s}\PY{l+s}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{ii}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{beers}\PY{p}{)}\PY{p}{,} \PY{n}{b1}\PY{p}{)}
            \PY{k}{for} \PY{n}{b2} \PY{o+ow}{in} \PY{n}{beers}\PY{p}{:}
                \PY{k}{if} \PY{n}{b1} \PY{o}{!=} \PY{n}{b2}\PY{p}{:}
                    \PY{n}{row} \PY{o}{=} \PY{p}{[}\PY{n}{b1}\PY{p}{,} \PY{n}{b2}\PY{p}{]} \PY{o}{+} \PY{n}{feature\PYZus{}distances}\PY{p}{(}\PY{n}{b1}\PY{p}{,}\PY{n}{b2}\PY{p}{)}
                    \PY{n}{sims}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{row}\PY{p}{)} 
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Calculating similarities for beer (1 / 11): Dale's Pale Ale
Calculating similarities for beer (2 / 11): Sierra Nevada Pale Ale
Calculating similarities for beer (3 / 11): Michelob Ultra
Calculating similarities for beer (4 / 11): Two Hearted Ale
Calculating similarities for beer (5 / 11): Natural Light
Calculating similarities for beer (6 / 11): Bud Light
Calculating similarities for beer (7 / 11): Fat Tire Amber Ale
Calculating similarities for beer (8 / 11): Coors Light
Calculating similarities for beer (9 / 11): Blue Moon Belgian White
Calculating similarities for beer (10 / 11): 90 Minute IPA
Calculating similarities for beer (11 / 11): Guinness Draught
    \end{Verbatim}

    Now, I create a new pandas dataframe to hold the similarities/distances.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{c}{\PYZsh{} CREATE NEW DATAFRAME FROM SIMILARITIES}
        \PY{n}{dists} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{sims}\PY{p}{,}\PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s}{\PYZsq{}}\PY{l+s}{Beer 1}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{Beer 2}\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{Palate}\PY{l+s}{\PYZsq{}}\PY{p}{,}\PY{l+s}{\PYZsq{}}\PY{l+s}{Aroma}\PY{l+s}{\PYZsq{}}\PY{p}{,}\PY{l+s}{\PYZsq{}}\PY{l+s}{Taste}\PY{l+s}{\PYZsq{}}\PY{p}{,}\PY{l+s}{\PYZsq{}}\PY{l+s}{ABV}\PY{l+s}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
        \PY{n}{dists}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}5}]:}             Beer 1                   Beer 2     Palate      Aroma      Taste  \textbackslash{}
        0  Dale's Pale Ale   Sierra Nevada Pale Ale  15.524175  15.564382  16.124515   
        1  Dale's Pale Ale           Michelob Ultra  29.832868  30.565503  31.626729   
        2  Dale's Pale Ale          Two Hearted Ale  17.248188  19.519221  18.621224   
        3  Dale's Pale Ale            Natural Light  23.622024  25.787594  26.162951   
        4  Dale's Pale Ale                Bud Light  38.108398  40.540104  41.542147   
        5  Dale's Pale Ale       Fat Tire Amber Ale  16.598193  17.392527  17.663522   
        6  Dale's Pale Ale              Coors Light  35.902646  38.301436  38.755645   
        7  Dale's Pale Ale  Blue Moon Belgian White  18.980253  18.907670  20.868637   
        8  Dale's Pale Ale            90 Minute IPA  19.855730  20.724382  21.230874   
        9  Dale's Pale Ale         Guinness Draught  24.382371  24.844516  25.258662   
        
                 ABV  
        0  23.658191  
        1  30.685827  
        2  13.756816  
        3  25.611716  
        4  42.284986  
        5  29.039800  
        6  40.495679  
        7  22.408704  
        8  74.035464  
        9  51.120055  
\end{Verbatim}
        
    \section{Find Closest Beer}\label{find-closest-beer}

Now let's say that I'm at a beer store that has all of these common
beers, except 90 Minute IPA. BOO! I really like 90 Minute IPA! But since
it isn't available, I'd like to find the next closest thing. The code
below finds the overall closest beer using the mean distance of each of
the beer features. The beer that is nearest to 90 Minute IPA will have
the lowest average distance.

First we define a function to calculate the average distance between two
beers based on their palate, aroma, taste, and abv.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{c}{\PYZsh{} CALCULATE OVERALL DISTANCES BETWEEN BEERS BASED ON FEATURE SIMILARITIES}
        \PY{n}{wfeatures} \PY{o}{=} \PY{p}{[}\PY{l+s}{\PYZsq{}}\PY{l+s}{Palate}\PY{l+s}{\PYZsq{}}\PY{p}{,}\PY{l+s}{\PYZsq{}}\PY{l+s}{Aroma}\PY{l+s}{\PYZsq{}}\PY{p}{,}\PY{l+s}{\PYZsq{}}\PY{l+s}{Taste}\PY{l+s}{\PYZsq{}}\PY{p}{,}\PY{l+s}{\PYZsq{}}\PY{l+s}{ABV}\PY{l+s}{\PYZsq{}}\PY{p}{]}
        \PY{k}{def} \PY{n+nf}{mean\PYZus{}distance}\PY{p}{(}\PY{n}{dists}\PY{p}{,} \PY{n}{beer1}\PY{p}{,} \PY{n}{beer2}\PY{p}{,} \PY{n}{w}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}Calculate average (weighted) distance between two beers}
        \PY{l+s+sd}{    takes DataFrame \PYZlt{}dists\PYZgt{} and two beer names as required input\PYZsq{}\PYZsq{}\PYZsq{}}
            \PY{n}{mask} \PY{o}{=} \PY{p}{(}\PY{n}{dists}\PY{p}{[}\PY{l+s}{\PYZsq{}}\PY{l+s}{Beer 1}\PY{l+s}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{n}{beer1}\PY{p}{)} \PY{o}{\PYZam{}} \PY{p}{(}\PY{n}{dists}\PY{p}{[}\PY{l+s}{\PYZsq{}}\PY{l+s}{Beer 2}\PY{l+s}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{n}{beer2}\PY{p}{)}
            \PY{n}{row} \PY{o}{=} \PY{n}{dists}\PY{p}{[}\PY{n}{mask}\PY{p}{]}
            \PY{k}{return} \PY{p}{(}\PY{n}{row}\PY{p}{[}\PY{n}{wfeatures}\PY{p}{]}\PY{o}{*}\PY{p}{(}\PY{n+nb}{pow}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{w}\PY{p}{)}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{.}\PY{n}{tolist}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
\end{Verbatim}

    Note that the function also allows us to weight each of the features,
giving it more importance in the distance calculation. For example,
perhaps, I really care about palate and ABV above all other things; I
could capture this by giving ABV and palate larger weights.

Now we calculate the average distance between 90 Minute IPA and all the
other beers in the store.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{n}{beer\PYZus{}test} \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{90 Minute IPA}\PY{l+s}{\PYZdq{}} \PY{c}{\PYZsh{} (ONE OF MY FAVORITES!)}
        \PY{n}{weights} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{]} \PY{c}{\PYZsh{} I PREFER PALATE AND ABV}
        
        \PY{n}{results} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{k}{for} \PY{n}{b} \PY{o+ow}{in} \PY{n}{beers}\PY{p}{:}
            \PY{k}{if} \PY{n}{b} \PY{o}{!=} \PY{n}{beer\PYZus{}test}\PY{p}{:}
                \PY{n}{results}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{p}{(}\PY{n}{beer\PYZus{}test}\PY{p}{,} \PY{n}{b}\PY{p}{,} \PY{n}{mean\PYZus{}distance}\PY{p}{(}\PY{n}{dists}\PY{p}{,}\PY{n}{beer\PYZus{}test}\PY{p}{,}\PY{n}{b}\PY{p}{,} \PY{n}{weights}\PY{p}{)}\PY{p}{)}\PY{p}{)}
                
        \PY{n}{results} \PY{o}{=} \PY{n+nb}{sorted}\PY{p}{(}\PY{n}{results}\PY{p}{,} \PY{n}{key}\PY{o}{=}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{x}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
        
        \PY{c}{\PYZsh{} CREATE NEW DATAFRAME HOLDING RANKINGS}
        \PY{n}{rankings} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{results}\PY{p}{,}\PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s}{\PYZsq{}}\PY{l+s}{Test Beer}\PY{l+s}{\PYZsq{}}\PY{p}{,}\PY{l+s}{\PYZsq{}}\PY{l+s}{Other Beer}\PY{l+s}{\PYZsq{}}\PY{p}{,}\PY{l+s}{\PYZsq{}}\PY{l+s}{Distance}\PY{l+s}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
        \PY{n}{srankings} \PY{o}{=} \PY{n}{rankings}\PY{o}{.}\PY{n}{sort}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{Distance}\PY{l+s}{\PYZsq{}}\PY{p}{)}\PY{p}{[}\PY{p}{[}\PY{l+s}{\PYZsq{}}\PY{l+s}{Other Beer}\PY{l+s}{\PYZsq{}}\PY{p}{,}\PY{l+s}{\PYZsq{}}\PY{l+s}{Distance}\PY{l+s}{\PYZsq{}}\PY{p}{]}\PY{p}{]}
        
        \PY{c}{\PYZsh{} PLOT RESULTS}
        \PY{n}{srankings}\PY{o}{.}\PY{n}{set\PYZus{}index}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{Other Beer}\PY{l+s}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{kind}\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{barh}\PY{l+s}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
        \PY{n}{title}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{Distances from }\PY{l+s+si}{\PYZpc{}s}\PY{l+s}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{beer\PYZus{}test}\PY{p}{)}\PY{p}{;}
        \PY{k}{print} \PY{l+s}{\PYZsq{}}\PY{l+s}{Closest beer to }\PY{l+s+si}{\PYZpc{}s}\PY{l+s}{ is }\PY{l+s+si}{\PYZpc{}s}\PY{l+s}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{beer\PYZus{}test}\PY{p}{,} \PY{n}{rankings}\PY{o}{.}\PY{n}{ix}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+s}{\PYZsq{}}\PY{l+s}{Other Beer}\PY{l+s}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Closest beer to 90 Minute IPA is Dale's Pale Ale
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Beer Reviews_files/Beer Reviews_14_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    OK, the results suggest that Dale's Pale Ale is closest to 90 Minute
IPA, followed closesly by Two Hearted Ale (another one of my
favorites!). Not so close is Bud Light and Coors Light. I'd have to say
that I agree with these suggested rankings!

    \section{Example II: Collaborative
Filtering}\label{example-ii-collaborative-filtering}

In the content-based filtering example above we chose beers that had
overlapping reviews from multiple reviewers. However, this generally
isn't going to be the case. Many beers are rated by only a few
reviewers, and it is generally unlikely that these reviewers will
overlap. To better demonstrate this, let's take a look at the first 5000
reviews in the data set:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{c}{\PYZsh{} DISPLAY A SUBSET OF THE RATINGS MATRIX}
        \PY{n}{df2} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{5000}\PY{p}{]}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
        
        \PY{c}{\PYZsh{} GET RID OF REPLICATED REVIEWS FOR PIVOT}
        \PY{n}{df2} \PY{o}{=} \PY{n}{df2}\PY{o}{.}\PY{n}{drop\PYZus{}duplicates}\PY{p}{(}\PY{p}{[}\PY{l+s}{\PYZsq{}}\PY{l+s}{review\PYZus{}profilename}\PY{l+s}{\PYZsq{}}\PY{p}{,}\PY{l+s}{\PYZsq{}}\PY{l+s}{beer\PYZus{}name}\PY{l+s}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
        \PY{n}{df2} \PY{o}{=} \PY{n}{df2}\PY{o}{.}\PY{n}{reindex}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{pivot}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{review\PYZus{}profilename}\PY{l+s}{\PYZsq{}}\PY{p}{,}\PY{l+s}{\PYZsq{}}\PY{l+s}{beer\PYZus{}name}\PY{l+s}{\PYZsq{}}\PY{p}{,}\PY{l+s}{\PYZsq{}}\PY{l+s}{review\PYZus{}overall}\PY{l+s}{\PYZsq{}}\PY{p}{)}
        \PY{n}{df2} \PY{o}{=} \PY{n}{df2}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{n}{NaN}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{)}
        \PY{n}{df2}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
        \PY{n}{feature\PYZus{}matrix} \PY{o}{=} \PY{n}{df2}\PY{o}{.}\PY{n}{values}
        \PY{k}{print} \PY{n}{feature\PYZus{}matrix}\PY{o}{.}\PY{n}{shape}
        \PY{c}{\PYZsh{} PLOT}
        \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,}\PY{l+m+mi}{12}\PY{p}{)}\PY{p}{)}\PY{p}{;}
        \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{feature\PYZus{}matrix}\PY{p}{,}\PY{n}{interpolation}\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{none}\PY{l+s}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
        \PY{n}{plt}\PY{o}{.}\PY{n}{colorbar}\PY{p}{(}\PY{p}{)}\PY{p}{;}
        \PY{n}{plt}\PY{o}{.}\PY{n}{set\PYZus{}cmap}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{hot}\PY{l+s}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
        \PY{n}{plt}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{tight}\PY{l+s}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{Beers}\PY{l+s}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{Users}\PY{l+s}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
(2137, 475)
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Beer Reviews_files/Beer Reviews_17_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    We see above that the ratings are very sparsely distributed, with lots
of zero (black) values. These zero values indicate beers that a beer
drinker may not have rated, but could potentially like (or dislike).
What we would like is a way of ``filling in'' these zero values. A
common way of performing this filling in is what is called low-rank
matrix factorization (LRMF).

In LRFM we assume that the ratings matrix
$R \in \mathbb{R}^{D \times B}$ ($D$ is the number of users/drinkers $B$
is the number of beers) can be approximated by the product of two
component matrices $U \in \mathbb{R}^{D \times F}$ and
$V \in \mathbb{R}^{B \times F}$, both of which have low rank $F$
(i.e.~both are tall):

$R = VU^T$

The component matrices can be calculated using an algorithm known as
Regularized Singular Value Decomposition (RSVD). Because the ratings
matrices for real-world problems are generally very large (millions of
rows and columns) and sparse (only 1-2\% of nonzero values), the RSVD
algorithm often uses a sparse data representation.

To perform the remainder of the collaborative filtering analysis, I
select the beers that have atleast 500 reviews (to scale down the
problem size for a laptop), and transform the data into a sparse vector
representation that can be used with the RSVD python module.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{c}{\PYZsh{} REMOVE BEERS WITH FEWER THAN 500 REVIEWS }
        \PY{c}{\PYZsh{} (MAKES THE PROBLEM SMALLER ON MY LAPTOP)}
        \PY{n}{n\PYZus{}reviews\PYZus{}min} \PY{o}{=} \PY{l+m+mi}{500}
        \PY{n}{df} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{beer\PYZus{}name}\PY{l+s}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{filter}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{x}\PY{o}{.}\PY{n}{count}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{n}{n\PYZus{}reviews\PYZus{}min}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{c}{\PYZsh{} CREATE UNIQUE USER AND BEER MAPPING}
         \PY{n}{user\PYZus{}mapping} \PY{o}{=} \PY{p}{\PYZob{}}\PY{n}{j}\PY{p}{:}\PY{n}{i} \PY{k}{for} \PY{n}{i}\PY{p}{,}\PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{review\PYZus{}profilename}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{\PYZcb{}}
         \PY{n}{beerid\PYZus{}mapping} \PY{o}{=} \PY{p}{\PYZob{}}\PY{n}{j}\PY{p}{:}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1} \PY{k}{for} \PY{n}{i}\PY{p}{,}\PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{beer\PYZus{}name}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{\PYZcb{}}
         
         \PY{c}{\PYZsh{} CREATE SPARSE DATA REPRESENTATION}
         \PY{c}{\PYZsh{} (beerID,userID,rating)}
         \PY{n}{records} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{k}{for} \PY{n}{ii} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{:}
             \PY{n}{tmp} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{n}{ii}\PY{p}{:}\PY{n}{ii}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{]}
             \PY{n}{records}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{p}{(}\PY{n}{beerid\PYZus{}mapping}\PY{p}{[}\PY{n}{tmp}\PY{o}{.}\PY{n}{beer\PYZus{}name}\PY{o}{.}\PY{n}{values}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{,} 
                             \PY{n}{user\PYZus{}mapping}\PY{p}{[}\PY{n}{tmp}\PY{o}{.}\PY{n}{review\PYZus{}profilename}\PY{o}{.}\PY{n}{values}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{,}
                             \PY{n}{tmp}\PY{o}{.}\PY{n}{review\PYZus{}overall}\PY{o}{.}\PY{n}{values}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{k+kn}{from} \PY{n+nn}{rsvd} \PY{k+kn}{import} \PY{n}{rating\PYZus{}t}\PY{p}{,} \PY{n}{RSVD}
         
         \PY{c}{\PYZsh{} CONVERT INTO PTHON RECARRAY FOR REGULARIZED SVD}
         \PY{n}{records} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{records}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{rating\PYZus{}t}\PY{p}{)}
         \PY{n}{n\PYZus{}reviewers} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{review\PYZus{}profilename}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         \PY{n}{n\PYZus{}beers} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{beer\PYZus{}name}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         \PY{k}{print} \PY{l+s}{\PYZsq{}}\PY{l+s+si}{\PYZpc{}d}\PY{l+s}{ distinct reviewers}\PY{l+s}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{n\PYZus{}reviewers}
         \PY{k}{print} \PY{l+s}{\PYZsq{}}\PY{l+s+si}{\PYZpc{}d}\PY{l+s}{ distinct beers}\PY{l+s}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{n\PYZus{}beers}
         \PY{k}{print} \PY{l+s}{\PYZsq{}}\PY{l+s+si}{\PYZpc{}d}\PY{l+s}{ total reviews}\PY{l+s}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n+nb}{len}\PY{p}{(}\PY{n}{records}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
25316 distinct reviewers
613 distinct beers
593932 total reviews
    \end{Verbatim}

    Thus, after removing beers with less than 500 reviews, we still have
nearly 600k reviews from 25316 reviewers and 613 beers. Below, I run
RSVD on the sparse representation of the ratings matrix, assuming a rank
of $F=10$

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{n}{F} \PY{o}{=} \PY{l+m+mi}{10} \PY{c}{\PYZsh{} ASSUMED RANK OF COMPONENT MATRICES}
         
         \PY{c}{\PYZsh{} RUN LOW\PYZhy{}RANK MATRIX FACTORIZATION}
         \PY{n}{model} \PY{o}{=} \PY{n}{RSVD}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{n}{F}\PY{p}{,} \PY{n}{records}\PY{p}{,} \PY{p}{(}\PY{n}{n\PYZus{}beers}\PY{p}{,}\PY{n}{n\PYZus{}reviewers}\PY{p}{)}\PY{p}{,} \PY{n}{maxEpochs}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#
             Factorizing                
\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#
factors=10, epochs=20, lr=0.001000, reg=0.011000, n=593932
Init TRMSE: 0.683222
----------------------------------------
epoche	train err	probe err	elapsed time
0	0.606077	0.000000	0.031238
1	0.587010	0.000000	0.031169
2	0.584024	0.000000	0.030205
3	0.582278	0.000000	0.033183
4	0.581051	0.000000	0.030448
5	0.580104	0.000000	0.036744
6	0.579332	0.000000	0.027876
7	0.578677	0.000000	0.027437
8	0.578107	0.000000	0.027492
9	0.577601	0.000000	0.028200
10	0.577145	0.000000	0.032120
11	0.576730	0.000000	0.027384
12	0.576348	0.000000	0.028080
13	0.575995	0.000000	0.027306
14	0.575665	0.000000	0.027512
15	0.575357	0.000000	0.027448
16	0.575066	0.000000	0.027276
17	0.574792	0.000000	0.028764
18	0.574531	0.000000	0.029337
19	0.574284	0.000000	0.029109
    \end{Verbatim}

    Now, let's take a look at the resulting component matrices calculated
using RSVD.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{c}{\PYZsh{}\PYZsh{} DISPLAY LOW\PYZhy{}RANK COMPONENT MATRICES}
         \PY{c}{\PYZsh{} V}
         \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{v}\PY{p}{,}\PY{n}{interpolation}\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{none}\PY{l+s}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
         \PY{n}{plt}\PY{o}{.}\PY{n}{set\PYZus{}cmap}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{seismic}\PY{l+s}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{colorbar}\PY{p}{(}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{tight}\PY{l+s}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
         \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{Low Rank Matrix V}\PY{l+s}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
         
         \PY{c}{\PYZsh{} U}
         \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{u}\PY{p}{,}\PY{n}{interpolation}\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{none}\PY{l+s}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
         \PY{n}{plt}\PY{o}{.}\PY{n}{colorbar}\PY{p}{(}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{tight}\PY{l+s}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
         \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{Low Rank Matrix U}\PY{l+s}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Beer Reviews_files/Beer Reviews_25_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Beer Reviews_files/Beer Reviews_25_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    We see that each matrix has only $F=10$ columns. Now, let's calculate
the approximated ratings matrix as $R = VU^T$

and take a look at its structure.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{c}{\PYZsh{} DISPLAY LOW\PYZhy{}RANK APPROXIMATION OF RATINGS}
         \PY{c}{\PYZsh{} \PYZdl{}VU\PYZca{}T\PYZdl{}}
         \PY{n}{all\PYZus{}rankings} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{v}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{u}\PY{o}{.}\PY{n}{T}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{set\PYZus{}cmap}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{hot}\PY{l+s}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{all\PYZus{}rankings}\PY{p}{,}\PY{n}{interpolation}\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{none}\PY{l+s}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
         \PY{n}{plt}\PY{o}{.}\PY{n}{colorbar}\PY{p}{(}\PY{p}{)}\PY{p}{;}
         \PY{n}{plt}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{tight}\PY{l+s}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{Beer Drinkers}\PY{l+s}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{Beers}\PY{l+s}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
         \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{Low\PYZhy{}rank approximation of ratings}\PY{l+s}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Beer Reviews_files/Beer Reviews_27_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    We see that most of the empty reviews have now been filled in. Now,
given this filled-in matrix, we can look at the predicted ratings that a
reviewer would give to all beers, despited the fact that only a few
reviews may have been provided. Specifically, the predicted reviews for
a beer reviewer will correspond to a row of $VU^T$

Below we'll choose a random reviewer (say, reviewer number 11) look at
their predicted beer ratings, and suggest the top 10 and bottom 10 beers
for that particular beer drinker.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{c}{\PYZsh{} CREATE REVERSED BEER\PYZhy{}ID MAPPING (i.e. ID \PYZhy{}\PYZhy{}\PYZgt{} BEER NAME)}
         \PY{n}{beerid\PYZus{}mapping} \PY{o}{=} \PY{p}{\PYZob{}}\PY{n}{i}\PY{p}{:}\PY{n}{j} \PY{k}{for} \PY{n}{i}\PY{p}{,}\PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{beer\PYZus{}name}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{\PYZcb{}}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{c}{\PYZsh{} PREDICT AN ARBITRARY USER\PYZsq{}S PREFERENCES}
         \PY{n}{user\PYZus{}num} \PY{o}{=} \PY{l+m+mi}{10}
         \PY{n}{user\PYZus{}rankings} \PY{o}{=} \PY{n}{all\PYZus{}rankings}\PY{p}{[}\PY{n}{user\PYZus{}num}\PY{p}{,}\PY{p}{:}\PY{p}{]}
         
         \PY{c}{\PYZsh{} SORT THE PREDICTED RATINGS}
         \PY{n}{sort\PYZus{}idx} \PY{o}{=} \PY{n+nb}{sorted}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{user\PYZus{}rankings}\PY{p}{)}\PY{p}{)}\PY{p}{,}\PY{n}{key}\PY{o}{=}\PY{n}{user\PYZus{}rankings}\PY{o}{.}\PY{n}{\PYZus{}\PYZus{}getitem\PYZus{}\PYZus{}}\PY{p}{)}
         
         \PY{c}{\PYZsh{} DISPLAY}
         \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{bar}\PY{p}{(}\PY{n}{arange}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{sort\PYZus{}idx}\PY{p}{)}\PY{p}{)}\PY{p}{,}\PY{n}{user\PYZus{}rankings}\PY{p}{[}\PY{n}{sort\PYZus{}idx}\PY{p}{]}\PY{p}{)}\PY{p}{;}
         \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{Drinker \PYZsh{}}\PY{l+s+si}{\PYZpc{}d}\PY{l+s}{\PYZsq{}}\PY{l+s}{s predicted beer rankings}\PY{l+s}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{user\PYZus{}num} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlim}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{n}{n\PYZus{}beers}\PY{p}{]}\PY{p}{)}\PY{p}{;}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{Ranked Beer}\PY{l+s}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{Beer Rating}\PY{l+s}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Beer Reviews_files/Beer Reviews_30_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}25}]:} \PY{c}{\PYZsh{} GET TOP AND BOTTOM RATED BEERS}
         \PY{n}{n\PYZus{}suggest} \PY{o}{=} \PY{l+m+mi}{10}
         \PY{n}{ranked\PYZus{}beers} \PY{o}{=} \PY{p}{[}\PY{n}{beerid\PYZus{}mapping}\PY{p}{[}\PY{n}{idx}\PY{p}{]} \PY{k}{for} \PY{n}{idx} \PY{o+ow}{in} \PY{n}{sort\PYZus{}idx}\PY{p}{]}
         \PY{k}{print} \PY{l+s}{\PYZdq{}}\PY{l+s}{Drinker \PYZsh{} }\PY{l+s+si}{\PYZpc{}d}\PY{l+s}{\PYZsq{}}\PY{l+s}{s likely }\PY{l+s+si}{\PYZpc{}d}\PY{l+s}{ most preferred beers}\PY{l+s}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{user\PYZus{}num} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{n\PYZus{}suggest}\PY{p}{)}
         \PY{k}{print} \PY{n}{ranked\PYZus{}beers}\PY{p}{[}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{n}{n\PYZus{}suggest}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
         \PY{k}{print} \PY{l+s}{\PYZsq{}}\PY{l+s}{\PYZsq{}}
         \PY{k}{print} \PY{l+s}{\PYZdq{}}\PY{l+s}{Drinker \PYZsh{} }\PY{l+s+si}{\PYZpc{}d}\PY{l+s}{\PYZsq{}}\PY{l+s}{s likely }\PY{l+s+si}{\PYZpc{}d}\PY{l+s}{ least preferred beers}\PY{l+s}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{user\PYZus{}num} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{n\PYZus{}suggest}\PY{p}{)}
         \PY{k}{print} \PY{n}{ranked\PYZus{}beers}\PY{p}{[}\PY{p}{:}\PY{n}{n\PYZus{}suggest}\PY{p}{]}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Drinker \# 11's likely 10 most preferred beers
['Trappist Westvleteren 12', 'Pliny The Elder', 'Pliny The Younger', 'Founders CBS Imperial Stout', 'Weihenstephaner Hefeweissbier', 'Trappist Westvleteren 8', 'Masala Mama India Pale Ale', 'Sculpin India Pale Ale', 'The Abyss']

Drinker \# 11's likely 10 least preferred beers
['Michelob Ultra', 'Samuel Adams Triple Bock', 'Natural Light', 'Bud Light', 'Corona Extra', 'Coors Light', 'Miller Genuine Draft', 'Samuel Adams Cranberry Lambic', 'Miller Lite', 'Kirin Ichiban']
    \end{Verbatim}

    The model suggests that this beer drinker really likes IPAs (and good
ones too from Russian River, like the Pliny's) and some stouts. The
reviewer however likely dislikes lighter, more large scale production
beers such as Bud/Michelobe/Coors Light. Sounds like my kind of beer
drinker!

    \section{Example III: Regression \& Beer Rating
Prediction}\label{example-iii-regression-beer-rating-prediction}

Another useful task in Data Science is being able to predict the outome
of some target variable (e.g.~product rating) contingent on some product
features. Here, I'll devise a simple regression model to predict overall
beer rating from the features aroma, appearance, palate, taste, and ABV.
I'll also assess which features are most important for predicting
overall rating.

First let's extract our features and perform some transformation that
will be useful for modeling. First we define a new data frame filled
with the model features, and use the dataframe to imput (fill in) NaN
values with the median of each respective feature.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{c}{\PYZsh{} LOAD DATA}
         \PY{n}{df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{/home/dustin/data/toy\PYZus{}datasets/beer\PYZus{}reviews/beer\PYZus{}reviews.csv}\PY{l+s}{\PYZsq{}}\PY{p}{)}
         
         \PY{c}{\PYZsh{} EXTRACT INDEPENDENT VARIABLES/FEATURES}
         \PY{n}{features} \PY{o}{=} \PY{p}{[}\PY{l+s}{\PYZsq{}}\PY{l+s}{review\PYZus{}aroma}\PY{l+s}{\PYZsq{}}\PY{p}{,}\PY{l+s}{\PYZsq{}}\PY{l+s}{review\PYZus{}appearance}\PY{l+s}{\PYZsq{}}\PY{p}{,}\PY{l+s}{\PYZsq{}}\PY{l+s}{review\PYZus{}palate}\PY{l+s}{\PYZsq{}}\PY{p}{,}\PY{l+s}{\PYZsq{}}\PY{l+s}{review\PYZus{}taste}\PY{l+s}{\PYZsq{}}\PY{p}{,}\PY{l+s}{\PYZsq{}}\PY{l+s}{beer\PYZus{}abv}\PY{l+s}{\PYZsq{}}\PY{p}{]}
         \PY{n}{dfmodel} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{n}{features}\PY{p}{]}\PY{p}{;}
         \PY{n}{dfmodel} \PY{o}{=} \PY{n}{dfmodel}\PY{o}{.}\PY{n}{fillna}\PY{p}{(}\PY{n}{dfmodel}\PY{o}{.}\PY{n}{median}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)} \PY{c}{\PYZsh{} IMPUTE MISSING VALUES WITH COLUMN MEDIAN}
         \PY{n}{dfmodel}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}18}]:}          review\_aroma  review\_appearance   review\_palate   review\_taste  \textbackslash{}
         count  1586614.000000     1586614.000000  1586614.000000  1586614.00000   
         mean         3.735636           3.841642        3.743701        3.79286   
         std          0.697617           0.616093        0.682218        0.73197   
         min          1.000000           0.000000        1.000000        1.00000   
         25\%          3.500000           3.500000        3.500000        3.50000   
         50\%          4.000000           4.000000        4.000000        4.00000   
         75\%          4.000000           4.000000        4.000000        4.50000   
         max          5.000000           5.000000        5.000000        5.00000   
         
                      beer\_abv  
         count  1586614.000000  
         mean         7.019214  
         std          2.275018  
         min          0.010000  
         25\%          5.300000  
         50\%          6.500000  
         75\%          8.400000  
         max         57.700000  
\end{Verbatim}
        
    Now we Gaussianize theses strictly-positive features by using a
compressive nonlinearity (i.e.~negative logarithm). Furthermore, we
standardize the model features so that each has zero mean and unit
variance. (Note we keep around the feature means and standard deviations
around, as I will use them to reverse the feature transformation durin
model interpretation).

Below we calculate the tranformed features and visualize them.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{c}{\PYZsh{} MODEL FEATURES}
         \PY{n}{model\PYZus{}features} \PY{o}{=} \PY{n}{dfmodel}\PY{o}{.}\PY{n}{values}\PY{p}{;}
         
         \PY{c}{\PYZsh{} GAUSSIANIZE}
         \PY{n}{model\PYZus{}features} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{+} \PY{n}{model\PYZus{}features}\PY{p}{)}\PY{p}{;}
         
         \PY{c}{\PYZsh{} KEEP MEANS AND STDS AROUND FOR INTERPRETATION LATERE}
         \PY{n}{feature\PYZus{}means} \PY{o}{=} \PY{n}{model\PYZus{}features}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
         \PY{n}{feature\PYZus{}std} \PY{o}{=} \PY{n}{model\PYZus{}features}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
         
         \PY{c}{\PYZsh{} STANDARDIZE VARIABLES}
         \PY{n}{model\PYZus{}features} \PY{o}{=} \PY{p}{(}\PY{n}{model\PYZus{}features} \PY{o}{\PYZhy{}} \PY{n}{feature\PYZus{}means} \PY{p}{)} \PY{o}{/} \PY{n}{feature\PYZus{}std}
         
         \PY{c}{\PYZsh{} DISPLAY THE FEATURE MATRIX}
         \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{model\PYZus{}features}\PY{p}{,}\PY{n}{interpolation}\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{none}\PY{l+s}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
         \PY{n}{plt}\PY{o}{.}\PY{n}{set\PYZus{}cmap}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{seismic}\PY{l+s}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{clim}\PY{p}{(}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{]}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{tight}\PY{l+s}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
         \PY{n}{plt}\PY{o}{.}\PY{n}{colorbar}\PY{p}{(}\PY{p}{)}\PY{p}{;}
         \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{Transformed Feature Matrix}\PY{l+s}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{n}{arange}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{features}\PY{p}{)}\PY{p}{)}\PY{p}{,}\PY{n}{features}\PY{p}{,} \PY{n}{rotation}\PY{o}{=}\PY{l+m+mi}{45}\PY{p}{)}\PY{p}{;}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Beer Reviews_files/Beer Reviews_36_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} \PY{c}{\PYZsh{} SANITY CHECK TO MAKE SURE FEATURES ARE CENTERD AND SCALED}
         \PY{n}{idx} \PY{o}{=} \PY{l+m+mi}{4}
         \PY{n}{plt}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{model\PYZus{}features}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{idx}\PY{p}{]}\PY{p}{,}\PY{n}{bins}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{)}\PY{p}{;}
         \PY{n}{title}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{Histogram of feature }\PY{l+s+si}{\PYZpc{}d}\PY{l+s}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{idx} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
         \PY{k}{print} \PY{l+s}{\PYZsq{}}\PY{l+s}{Features Means: }\PY{l+s}{\PYZsq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{model\PYZus{}features}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}
         \PY{k}{print} \PY{l+s}{\PYZsq{}}\PY{l+s}{Features Variances: }\PY{l+s}{\PYZsq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{model\PYZus{}features}\PY{o}{.}\PY{n}{var}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Features Means: [  5.09083994e-11   3.27769103e-11   4.92104533e-11   3.24433521e-11
  -9.66557237e-12]
Features Variances: [ 1.  1.  1.  1.  1.]
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Beer Reviews_files/Beer Reviews_37_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Ok, the features looks good; they all have zero mean and unit variance.
Now let's extract the target values. (Once and perform similar
preprocessing sucha as centering and scaling, but I found that it makes
little difference in model accuracy).

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} \PY{c}{\PYZsh{} EXTRACT TARGET VARIABLES}
         \PY{n}{dftargets} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{l+s}{\PYZsq{}}\PY{l+s}{review\PYZus{}overall}\PY{l+s}{\PYZsq{}}\PY{p}{]}
         \PY{n}{dftargets}\PY{o}{.}\PY{n}{fillna}\PY{p}{(}\PY{n}{dftargets}\PY{o}{.}\PY{n}{median}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)} \PY{c}{\PYZsh{} IMPUTE WITH MEDIAN}
         \PY{n}{targets} \PY{o}{=} \PY{n}{dftargets}\PY{o}{.}\PY{n}{values}\PY{p}{;}
         
         \PY{c}{\PYZsh{} STANDARDIZE TARGET VARIABLES}
         \PY{c}{\PYZsh{}z\PYZus{}score = lambda x: (x \PYZhy{} x.mean(0)) / x.std(0);}
         \PY{c}{\PYZsh{}targets = z\PYZus{}score(targets)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{targets}\PY{p}{,}\PY{n}{bins}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{)}\PY{p}{;}
         \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{Target  Values Histogram}\PY{l+s}{\PYZsq{}}\PY{p}{)}
         
         \PY{k}{print} \PY{l+s}{\PYZsq{}}\PY{l+s}{Target Mean: }\PY{l+s}{\PYZsq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{targets}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         \PY{k}{print} \PY{l+s}{\PYZsq{}}\PY{l+s}{Target Variance: }\PY{l+s}{\PYZsq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{targets}\PY{o}{.}\PY{n}{var}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Target Mean: 3.81558085331
Target Variance: 0.519295549419
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Beer Reviews_files/Beer Reviews_39_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Alright! Now that we have our model features and targets, let's
construct a regression model. This is a pretty ideal modeling situation,
given that there are so many observations and so few features. Therefore
I'll likely not need a super fancy model. I'll choose to just use Linear
Regression for now.

One thing that is helpful for interpreting Linear Regression models is
to add some form of regularizer penalty. In principle regularizers such
as the Lasso penalty perform feature selection by influencing the
distribution of model parameters to be sparse (i.e.~many equal to zero).
However, regularizers generally require setting a hyperparameter that
determines the strength of the regularizer's influence. Since we don't
know a priori the correct hyperparameter setting, we'll try out a bunch,
validating each one using cross validation. Specifically, we'll hold out
random portions of the training data, and use it as a cross-validation
set. We'll then estimate the model for a given hyperparameter setting,
then assess the model accuracy on the cross-valiation set. This process
is repeated many times, and the hyperparameter that offers the most
accurate model is kept.

Scikits-learn offers some nice functionality for performing such
hyperparameter search. In particular, I'll use the GridSearchCV module,
which automatically selects hyperparameters to test from a provided grid
of possible values.

Before model fitting, I'll shuffle the observation order remove
redundant local structure in the data; this will keep the optimization
algorithm used to estimate the coefficients from getting stuck in the
wrong location in parameter space. I'll also remove the first 1000
observations and use them as as a testing set used to asses the best
cross-validated model's performance. The testing set is important as it
is an indicator of how the final model will perform on arbitrary data.

The entire model fitting analysis is below:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} \PY{k+kn}{from} \PY{n+nn}{time} \PY{k+kn}{import} \PY{n}{time}
         \PY{k+kn}{from} \PY{n+nn}{operator} \PY{k+kn}{import} \PY{n}{itemgetter}
         \PY{k+kn}{from} \PY{n+nn}{sklearn.linear\PYZus{}model} \PY{k+kn}{import} \PY{n}{Lasso}
         \PY{k+kn}{from} \PY{n+nn}{sklearn.grid\PYZus{}search} \PY{k+kn}{import} \PY{n}{GridSearchCV}
         \PY{k+kn}{from} \PY{n+nn}{sklearn.utils} \PY{k+kn}{import} \PY{n}{shuffle}
         \PY{k+kn}{from} \PY{n+nn}{sklearn.metrics} \PY{k+kn}{import} \PY{n}{r2\PYZus{}score}
         
         \PY{k}{def} \PY{n+nf}{report}\PY{p}{(}\PY{n}{grid\PYZus{}scores}\PY{p}{,} \PY{n}{n\PYZus{}top}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}For reporting best models\PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{n}{top\PYZus{}scores} \PY{o}{=} \PY{n+nb}{sorted}\PY{p}{(}\PY{n}{grid\PYZus{}scores}\PY{p}{,} \PY{n}{key}\PY{o}{=}\PY{n}{itemgetter}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{reverse}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{n}{n\PYZus{}top}\PY{p}{]}
             \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{score} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{top\PYZus{}scores}\PY{p}{)}\PY{p}{:}
                 \PY{k}{print}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{Model with rank: \PYZob{}0\PYZcb{}}\PY{l+s}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{i} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
                 \PY{k}{print}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{Mean validation score: \PYZob{}0:.3f\PYZcb{} (std: \PYZob{}1:.3f\PYZcb{})}\PY{l+s}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}
                       \PY{n}{score}\PY{o}{.}\PY{n}{mean\PYZus{}validation\PYZus{}score}\PY{p}{,}
                       \PY{n}{np}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{n}{score}\PY{o}{.}\PY{n}{cv\PYZus{}validation\PYZus{}scores}\PY{p}{)}\PY{p}{)}\PY{p}{)}
                 \PY{k}{print}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{Parameters: \PYZob{}0\PYZcb{}}\PY{l+s}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{score}\PY{o}{.}\PY{n}{parameters}\PY{p}{)}\PY{p}{)}
                 \PY{k}{print}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{\PYZdq{}}\PY{p}{)}
         
         \PY{c}{\PYZsh{} SHUFFLE DATA}
         \PY{n}{model\PYZus{}features}\PY{p}{,} \PY{n}{targets} \PY{o}{=} \PY{n}{shuffle}\PY{p}{(}\PY{n}{model\PYZus{}features}\PY{p}{,} \PY{n}{targets}\PY{p}{)}
         
         \PY{n}{train\PYZus{}features} \PY{o}{=} \PY{n}{model\PYZus{}features}\PY{p}{[}\PY{l+m+mi}{1000}\PY{p}{:}\PY{p}{]}
         \PY{n}{train\PYZus{}targets} \PY{o}{=} \PY{n}{targets}\PY{p}{[}\PY{l+m+mi}{1000}\PY{p}{:}\PY{p}{]}
         
         \PY{c}{\PYZsh{} TEST ON FIRST 1000 OBSERVATIONS}
         \PY{n}{test\PYZus{}features} \PY{o}{=} \PY{n}{model\PYZus{}features}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{1000}\PY{p}{]}
         \PY{n}{test\PYZus{}targets} \PY{o}{=} \PY{n}{targets}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{1000}\PY{p}{]}
                 
         \PY{c}{\PYZsh{}\PYZsh{}\PYZsh{} INIT LINEAR REGRESSION MODEL}
         \PY{k}{print} \PY{l+s}{\PYZsq{}}\PY{l+s}{\PYZus{}}\PY{l+s}{\PYZsq{}}\PY{o}{*}\PY{l+m+mi}{20} \PY{o}{+} \PY{l+s}{\PYZsq{}}\PY{l+s}{ Linear Regression Model }\PY{l+s}{\PYZsq{}} \PY{o}{+} \PY{l+s}{\PYZsq{}}\PY{l+s}{\PYZus{}}\PY{l+s}{\PYZsq{}}\PY{o}{*}\PY{l+m+mi}{20} \PY{o}{+} \PY{l+s}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s}{\PYZsq{}}      
         \PY{n}{regr} \PY{o}{=} \PY{n}{Lasso}\PY{p}{(}\PY{p}{)}
         
         \PY{c}{\PYZsh{} PARAMETER GRID FOR REGULARIZED LINEAR REGRESSION}
         \PY{n}{param\PYZus{}grid\PYZus{}lr} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s}{\PYZdq{}}\PY{l+s}{alpha}\PY{l+s}{\PYZdq{}}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{logspace}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{\PYZcb{}}
         
         \PY{c}{\PYZsh{} RUN PARAMETER ESTIMATION}
         \PY{n}{n\PYZus{}iter\PYZus{}search} \PY{o}{=} \PY{l+m+mi}{10}
         
         \PY{c}{\PYZsh{} GRID SEARCH OBJECT}
         \PY{n}{random\PYZus{}search\PYZus{}lr} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{regr}\PY{p}{,} \PY{n}{param\PYZus{}grid}\PY{o}{=}\PY{n}{param\PYZus{}grid\PYZus{}lr}\PY{p}{)}
         
         \PY{c}{\PYZsh{} FIT MODELS}
         \PY{n}{start} \PY{o}{=} \PY{n}{time}\PY{p}{(}\PY{p}{)}
         \PY{n}{random\PYZus{}search\PYZus{}lr}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{train\PYZus{}features}\PY{p}{,} \PY{n}{train\PYZus{}targets}\PY{p}{)}
         
         \PY{k}{print}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{Linear Regression Estimation using RandomizedSearchCV took }\PY{l+s+si}{\PYZpc{}.2f}\PY{l+s}{ seconds for }\PY{l+s+si}{\PYZpc{}d}\PY{l+s}{ candidates models}\PY{l+s}{\PYZdq{}}
               \PY{l+s}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s}{Best Parameter settings:}\PY{l+s}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{p}{(}\PY{n}{time}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{start}\PY{p}{)}\PY{p}{,} \PY{n}{n\PYZus{}iter\PYZus{}search}\PY{p}{)}\PY{p}{)}
         \PY{k}{print} \PY{l+s}{\PYZsq{}}\PY{l+s}{\PYZus{}}\PY{l+s}{\PYZsq{}}\PY{o}{*}\PY{l+m+mi}{25} \PY{o}{+} \PY{l+s}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s}{\PYZsq{}}
         
         \PY{n}{report}\PY{p}{(}\PY{n}{random\PYZus{}search\PYZus{}lr}\PY{o}{.}\PY{n}{grid\PYZus{}scores\PYZus{}}\PY{p}{)}
         \PY{n}{model} \PY{o}{=} \PY{n}{random\PYZus{}search\PYZus{}lr}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ Linear Regression Model \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

Linear Regression Estimation using RandomizedSearchCV took 18.63 seconds for 10 candidates models
Best Parameter settings:
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

Model with rank: 1
Mean validation score: 0.672 (std: 0.000)
Parameters: \{'alpha': 1.0000000000000001e-05\}

Model with rank: 2
Mean validation score: 0.672 (std: 0.000)
Parameters: \{'alpha': 4.6415888336127818e-05\}

Model with rank: 3
Mean validation score: 0.672 (std: 0.000)
Parameters: \{'alpha': 0.00021544346900318823\}
    \end{Verbatim}

    We see that the grid search determines a low amount of regularization
for this problem (i.e.~performing little feature selection). This is not
at all surprising given the fact that there are so few features. We also
see that the best models generally predict 67 percent of the variance in
the validation set responses. This corresponds to a correlation
coefficient of 0.81, not too shabby for such a simple model.

But what we really care about is how well the model generalizes to
completely new data (i.e.~the testing set). Let's see how it does on the
testing set:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}23}]:} \PY{n}{r2} \PY{o}{=} \PY{n}{r2\PYZus{}score}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{test\PYZus{}features}\PY{p}{)}\PY{p}{,}\PY{n}{test\PYZus{}targets}\PY{p}{)}
         \PY{n}{cc} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{r2}\PY{p}{)}
         \PY{k}{print} \PY{l+s}{\PYZsq{}}\PY{l+s}{Percent of ratings variance explained on testing set is }\PY{l+s+si}{\PYZpc{}1.2f}\PY{l+s}{ (correlation }\PY{l+s+si}{\PYZpc{}1.2f}\PY{l+s}{)}\PY{l+s}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{r2}\PY{p}{,} \PY{n}{cc}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Percent of ratings variance explained on testing set is 0.57 (correlation 0.76)
    \end{Verbatim}

    Over fifty percent of variance explained by 5 predictor variables isnt
too bad. In fact, the chance of getting this prediction accuracy on the
1000 testing observations is basically zero (pvalue \textless{} 1e-12).

Now, let's look at what the model represents, in terms of the beer
features. To do so, I'll project the model coefficients back into the
original space by applying a reverse of the standardization and
Gaussianization transform applied to the features. The results are
plotted below:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}24}]:} \PY{c}{\PYZsh{} RETREIVE MODEL COEFFICIENTS AND REVERSE NORMALIZATION AND}
         \PY{c}{\PYZsh{} COMPRESSION}
         \PY{n}{yy} \PY{o}{=} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{)}\PY{p}{)}
         \PY{n}{coeff} \PY{o}{=} \PY{n}{exp}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{o}{*}\PY{n}{feature\PYZus{}std}\PY{p}{)} \PY{o}{+} \PY{n}{feature\PYZus{}means}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{barh}\PY{p}{(}\PY{n}{yy}\PY{p}{,}\PY{n}{coeff}\PY{p}{)}\PY{p}{;}
         \PY{n}{plt}\PY{o}{.}\PY{n}{yticks}\PY{p}{(}\PY{n}{yy}\PY{p}{,}\PY{n}{features}\PY{p}{)}\PY{p}{;}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Beer Reviews_files/Beer Reviews_45_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    The inverse-transformed coefficients suggest that overall beer ratings
are most related to a beer's rated taste, followed closely by the beer's
palate and aroma. It turns out that ABV is less related to the overall
rating than the remaining four features. Cool!

    \section{Other things to try}\label{other-things-to-try}

The regression analyses could be extended in a number of ways including:
- Hand-designed features. Perhaps we could hang-engineer better features
for predicing overall rating. For example, including interaction terms
or the time of review. - Other, more nonlinear model. Perphaps the
nonlinear transformed provided by logarithm is incorrect. Other models
such as a neural network or decision trees could determine a more
fitting (pun intended) nonlinear transformation of the input features. -
Feature learning. It seems that a lot of the features used are redundant
(i.e.~similar model weights). Perhaps the model can be improved by using
features derived using some factor analysis or feature learning approach
such as Principal or Independent Components analysis (PCA or ICA).


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
